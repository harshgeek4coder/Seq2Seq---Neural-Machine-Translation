# -*- coding: utf-8 -*-
"""nmt with attention - BiLstms .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cslvbyQeB6ytlIOez7SqU-ws55AUbJmp
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

'''
BATCHSIZE = 128
EPOCH = 35
LATENT_DIM = 500
LATENT_DIM_DECODER = LATENT_DIM
SAMPLES = 23000
MAX_WORD_NUM = SAMPLES
MAX_SEQ_LEN = 100
EMBEDDING = MAX_SEQ_LEN
'''

batch_size=128
epoch=35
latent_dim=500
max_samples=20000
max_seq_len=100

data_path='/content/gdrive/MyDrive/Seq2Seq Content/data/fra.txt'

lines=pd.read_table(data_path,names=['eng','fr','_'])
lines

# sos -> STARTING OF STRING / EOS -> END OF STRING : 
#FOR TRAINING SEQ2SEQ ATTENTION :
#lines.fr=lines.fr.apply(lambda x:"<sos> "+x+" <eos>")
lines

eng_texts=list(lines.eng)[:max_samples]
fr_texts_input=list("<sos> "+lines.fr)[:max_samples]
fr_texts_output=list(lines.fr+" <eos>")[:max_samples]

fr_texts_output



len(eng_texts),len(fr_texts_input),len(fr_texts_output)



# Text PreProcessing  :

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# FOR INPUTS : ENGLISH WORDS
#Tokenizing Words upto Max_samples defined earlier
tok_in=Tokenizer(num_words=max_samples)
tok_in.fit_on_texts(eng_texts)

#Getting Word Index -> Words associated with Unique Integers :
word2idx_in=tok_in.word_index

#Converting Individual Sentances to Integer Based Sequences
eng_seq=tok_in.texts_to_sequences(eng_texts)
eng_seq

#Print The Largest Sequence Length
max_in_len=max(len(s)for s in eng_seq)

print("Max Length of Sequence : ",max_in_len)
print("Total Number of Unique Tokens : ",len(word2idx_in))



#FOR DECODER : FRENCH WORDS ->

tok_out=Tokenizer(num_words=max_samples)
tok_out.fit_on_texts(fr_texts_input+fr_texts_output)

word2idx_out=tok_out.word_index

french_seq_in=tok_out.texts_to_sequences(fr_texts_input)
french_seq_out=tok_out.texts_to_sequences(fr_texts_output)

max_out_len=max(len(s)for s in french_seq_out)

total_out_words=len(word2idx_out)+1

print("Total Output Tokens : ",len(word2idx_out))
print("Max Length of Output Sequence : ",max_out_len)



# PADDING THE SEQUENCES FOR UNIFORMITY :
eng_seq_padded=pad_sequences(eng_seq,maxlen=max_in_len,padding='post')

fr_seq_in_padded=pad_sequences(french_seq_in,maxlen=max_out_len,padding='post')

fr_seq_out_padded=pad_sequences(french_seq_out,maxlen=max_out_len,padding='post')



fr_seq_in_padded

fr_seq_out_padded



#Creating Custom Word Embedding with the help of Glove Algorithm :

word_vec={}
glove_path='/content/gdrive/MyDrive/Seq2Seq Content/Glove Embeddings/glove.6B.100d.txt'
print("Starting Extaction of Glove Matrix: ")
with open(glove_path,encoding='utf-8') as f:
    for line in f:
        data=line.split()
        word=data[0]
        vec=np.asarray(data[1:],dtype='float32')
        word_vec[word]=vec
        
print("Finshed")

word_vec

# For just unique tokens , we need unique vectors in 100 D 
word_num=min(max_samples,len(word2idx_in)+1)

word_embedding=np.zeros((word_num,max_seq_len))

for token,idx in word2idx_in.items():
    if idx<max_samples:
        word_vector=word_vec.get(token)
        if word_vector is not None:
            word_embedding[idx]=word_vector

word_embedding



#Creating Target Matrix -> padded output target sequence -> one-hot encoding.

french_target_onehot=np.zeros((len(eng_texts),max_out_len,len(word2idx_out)+1),dtype='float32')

for idx,tokenvector in enumerate(fr_seq_out_padded):
    for tokenindex,token in enumerate(tokenvector):
        if(token>0):
            french_target_onehot[idx,tokenindex,token]=1



french_target_onehot.shape



#Model Defination :

from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM,Bidirectional,Dense,Concatenate,Embedding,Activation,Dot,Lambda,Input,RepeatVector

#1. Embedding Set-up : Embedding Layer Comprises of Standard Embedding shapes -> Total Number of Words , Max sequence length
#,Weights [Obtained from Glove Custom Word Embedding] and Max Input Length

embedding=Embedding(word_num,max_seq_len,weights=[word_embedding],input_length=max_in_len)

#2. Encoder Layer :



input_encoder_layer=Input(shape=(max_in_len,))
embed_encoder=embedding(input_encoder_layer)

encoder=Bidirectional(LSTM(latent_dim,return_sequences=True))
encoder_out=encoder(embed_encoder)

#3. Decoder Layer :
#3.A -> Decoder Input Layer :

input_decoder_layer=Input(shape=(max_out_len,))
embed_decoder=Embedding(total_out_words,max_seq_len)
decoder_input=embed_decoder(input_decoder_layer)



#3. Decoder Model After Attention :

decoder=LSTM(latent_dim,return_state=True)
decoder_dense=(Dense(total_out_words,activation='softmax'))


#States :

s0=Input(latent_dim,)
c0=Input(latent_dim,)

import tensorflow.keras as K



#Custom Attention Functions :

#Compute Softmax :
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)
    

repeat_attn=RepeatVector(max_in_len)
concat_attn=Concatenate(axis=-1)
dense1_attn=Dense(10,activation='tanh')
dense2_attn=Dense(1,activation='softmax')
dot_attn=Dot(axes=1)

                  
def iter_attention(h,prev_out):
    prev_output_repeat=repeat_attn(prev_out)
    total=concat_attn([h,prev_output_repeat])
    d1=dense1_attn(total)
    alpha_layer=dense2_attn(d1)
    
    context=dot_attn([alpha_layer,h])
                  
    return context

import tensorflow as tf
import tensorflow.keras as K



#Execute encoder-decoder with attention -> (with teacher forcing) -> to get a list of outputs:


s=s0
c=c0

all_output=[]

for t in range(max_out_len):
    #1. - > to get context vector
    context_vector=iter_attention(encoder_out,s)
    
    #2. Get Previous Word for Teacher Enforcing Training From Decoder's Input :
    select_layer=Lambda(lambda x:x[:,t:t+1])
    prev_word=select_layer(decoder_input)
    
    #3. Create Concatenate Object with axis =2 -> For two features ; Concatenate Context and Previous word
    concat_2=Concatenate(axis=2)
    decoder_in_concat=concat_2([context_vector,prev_word])
    
    #4. Get Outputs(Predictions) , cell state ,hidden state 
    pred,s,c=decoder(decoder_in_concat,initial_state=[s,c])
    pred=decoder_dense(pred)
    
    all_output.append(pred)

all_output



#The output needs to be stacked for -> the network's output. 
#Also, we need batchsize N to be the first dimension, -> thus a permutation of dimensions is required. 
#Afterwards, the model can be defined.

def stack(outputs):
    outputs=tf.stack(outputs)
    return tf.keras.backend.permute_dimensions(outputs,pattern=(1,0,2))

stack_layer=Lambda(stack)
all_output=stack_layer(all_output)



#Final Model Compiling :

attention_model=Model(inputs=[input_encoder_layer,input_decoder_layer,s0,c0],outputs=all_output)

attention_model.summary()



initial_s=np.zeros((len(eng_seq_padded),latent_dim))
initial_c=np.zeros((len(eng_seq_padded),latent_dim))

attention_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics='accuracy')

history=attention_model.fit(x=[eng_seq_padded,fr_seq_out_padded,initial_s,initial_c],y=french_target_onehot,
                            batch_size=batch_size,epochs=epoch,validation_split=0.2)



plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.show()

