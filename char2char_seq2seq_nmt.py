# -*- coding: utf-8 -*-
"""char2char-seq2seq -nmt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ywet98s1M1BspAcBoQOAKREXu_xjv_Oj
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import tensorflow as tf

!ls

!cd gdrive

!ls

data_path='/content/gdrive/MyDrive/Seq2Seq Content/data/fra.txt'

lines=open(data_path).read().split('\n')

lines

len(lines)



num_samples=50000

min(num_samples,len(lines))

for line in lines[0:10000]:
    x=line.split('\t')
x

input_texts=[]
target_texts=[]

input_chars=set()
target_chars=set()

for line in lines[:min(num_samples,len(lines)-1)]:
    input_text,target_text,_=line.split('\t')
    # For Decoder to understand - > Starting TAG with \t and Ending TAG with \n
    
    target_text='<GO>' + target_text + '<END>'
    
    input_texts.append(input_text)
    target_texts.append(target_text)
    
    for char in input_text:
        if char not in input_chars:
            input_chars.add(char)
    
    for char in target_text:
        if char not in target_chars:
            target_chars.add(char)

input_texts,target_texts



input_chars,target_chars



input_chars=sorted(list(input_chars))
target_chars=sorted(list(target_chars))

num_encoder_tokens=len(input_chars)
num_decoder_tokens=len(target_chars)


max_encoder_seq_len=max([len(txt) for txt in input_texts])
max_decoder_seq_len=max([len(txt) for txt in target_texts])



print('Total Number of samples:', len(input_texts))
print('Number of unique input tokens:', num_encoder_tokens)
print('Number of unique output tokens:', num_decoder_tokens)
print('Max sequence length for inputs:', max_encoder_seq_len)
print('Max sequence length for outputs:', max_decoder_seq_len)

#dict([i for i in enumerate(input_chars)])

#Mapping Dictionaries :
input_token_index=dict([(char,i) for i,char in enumerate(input_chars)])
target_token_index=dict([char,i] for i,char in enumerate(target_chars))

input_token_index['a'],input_token_index['A']

input_token_index,target_token_index

target_token_index

#Important shaping of inputs :

encoder_input_data=np.zeros(
    (len(input_texts),max_encoder_seq_len,num_encoder_tokens),
    dtype='float32'
)

decoder_input_data=np.zeros(
(len(input_texts),max_decoder_seq_len,num_decoder_tokens),
    dtype='float32'
)

decoder_target_data=np.zeros(
(len(input_texts),max_decoder_seq_len,num_decoder_tokens),
    dtype='float32'
)

decoder_input_data.shape

for i in zip(input_texts,target_texts):
    print(i)

for i in enumerate(zip(input_texts,target_texts)):
    print(i)



for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):
    for t,char in enumerate(input_text):
        encoder_input_data[i,t,input_token_index[char]]=1
        
    for t,char in enumerate(target_text):
        
        decoder_input_data[i,t,target_token_index[char]]=1        
        if t>0:
            decoder_target_data[i,t-1,target_token_index[char]]=1



import tensorflow as tf
from tensorflow.keras.layers import Input,LSTM,Dense
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping

batch_size = 64  # Batch size for training.
epochs = 100  # Number of epochs to train for.
latent_dim = 256

#Main Model For Training   :
encoder_inputs=Input(shape=(None,num_encoder_tokens))
encoder=LSTM(latent_dim,return_state=True)
encoder_output,state_h,state_c=encoder(encoder_inputs)


encoder_states=[state_h,state_c]

decoder_inputs=Input(shape=(None,num_decoder_tokens))
decoder=LSTM(latent_dim,return_sequences=True,return_state=True)

decoder_outputs,_,_=decoder(decoder_inputs,initial_state=encoder_states)

decoder_dense=Dense(num_decoder_tokens,activation='softmax')
decoder_outputs=decoder_dense(decoder_outputs)

model=Model([encoder_inputs,decoder_inputs],decoder_outputs)

optim=tf.keras.optimizers.RMSprop()
tf.keras.backend.clear_session()
model.compile(optimizer=optim,loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()
history=model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=batch_size,
                  epochs=epochs,validation_split=0.2,callbacks=EarlyStopping(monitor='val_loss',patience=5))

#Inference Model For Decoding Sentances :

encoder_model=Model(encoder_inputs,encoder_states)

decoder_state_input_h=Input(shape=(latent_dim,))
decoder_state_input_c=Input(shape=(latent_dim,))
decoder_state_inputs=[decoder_state_input_h,decoder_state_input_c]

decoder_outputs,state_h,state_c=decoder([decoder_inputs,initial_state=decoder_state_inputs])
decoder_states=[state_h,state_c]
decoder_outputs=decoder_dense(decoder_outputs)

decoder_model=Model(
[decoder_inputs]+decoder_state_inputs,
[decoder_outputs]+decoder_states 
)

reverse_input_char_index = dict(
    (i, char) for char, i in input_token_index.items())
reverse_target_char_index = dict(
    (i, char) for char, i in target_token_index.items())

def decode_sequence(input_seq):
    state_values=encoder_model.predict(input_seq)
    target_seq=np.zeroes((1,1,num_decoder_tokens))
    target_seq[0,0,target_token_index['<GO>']]=1
    
    stop_condition=False
    
    decoded_sentence=''
    
    while not stop_condition:
        output_tokens,h,c=decoder_model.predict(
        [target_seq]+states_value
        )
        
        sampled_token_index=np.argmax(output_tokens[0,-1,:])
        sampled_char=reverse_target_char_index[sampled_token_index]
        decoded_sentence=decoded_sentence+sampled_char
        
        
        if (sampled_char == '\n' or
           len(decoded_sentence) > max_decoder_seq_length):
            stop_condition = True
            
            
        target_seq=np.zeroes((1,1,num_decoder_tokens))
        target_seq[0,0,sampled_token_index]=1
        
        states_value=[h,c]
        
        
    return decoded_sentence